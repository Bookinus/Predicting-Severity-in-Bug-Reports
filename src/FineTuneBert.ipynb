{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes the string values in the \"Severity\" column into numerical values, storing the result in a new column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv('../DataSet/CleanDataSet.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"EncodedSeverity\"] = label_encoder.fit_transform(df[\"Severity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "INFERENCE_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 3e-5\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_CLUSTERS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing and returning data in a format that can be directly used for training a model\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text_description = dataframe.TextDescription\n",
    "        self.targets = dataframe.EncodedSeverity\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_description)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_description = str(self.text_description[index])\n",
    "        text_description = \" \".join(text_description.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text_description,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.pretrainedLayer = transformers.BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropOutLayer = torch.nn.Dropout(0.3)\n",
    "        self.linearLayer = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output_1= self.pretrainedLayer(input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.dropOutLayer(output_1)\n",
    "        output = self.linearLayer(output_2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (pretrainedLayer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropOutLayer): Dropout(p=0.3, inplace=False)\n",
       "  (linearLayer): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializes the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader\n",
    "\n",
    "train_size = 0.7\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the DataLoader objects for training and testing datasets, specifying how the data should be loaded during training and evaluation\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "# Adapting weights to balance an unbalanced dataset \n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=\n",
    "                                                 np.unique(df.EncodedSeverity),\n",
    "                                                 y=df.EncodedSeverity)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(class_weights)(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    fin_outputs = []\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "    return fin_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  1.6038150787353516\n",
      "Epoch: 1, Loss:  1.943310260772705\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.argmax(outputs, dim=1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    print(\"Sample Predictions:\", fin_outputs[:20])  # Print first 20 predictions\n",
    "    print(\"Actual Labels:\", fin_targets[:20]) \n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions: [4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4]\n",
      "Actual Labels: [4, 4, 4, 4, 4, 4, 2, 5, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4]\n",
      "Accuracy Score = 0.7233082706766917\n",
      "Balanced Accuracy Score = 0.2639721912657049\n",
      "F1 Score (Micro) = 0.7233082706766917\n",
      "F1 Score (Macro) = 0.23765277052732323\n",
      "Sample Predictions: [4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 3, 0, 4, 4, 2, 4, 4, 4, 4, 4]\n",
      "Actual Labels: [3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4]\n",
      "Accuracy Score = 0.7233082706766917\n",
      "Balanced Accuracy Score = 0.2639721912657049\n",
      "F1 Score (Micro) = 0.7233082706766917\n",
      "F1 Score (Macro) = 0.23765277052732323\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs)\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    b_accuracy = metrics.balanced_accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"Balanced Accuracy Score = {b_accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../Model/fine_tuned_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (pretrainedLayer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropOutLayer): Dropout(p=0.3, inplace=False)\n",
       "  (linearLayer): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the pretrained model\n",
    "pretrained_model = BERTClass()\n",
    "pretrained_model.to(device)\n",
    "\n",
    "# Load the saved model\n",
    "pretrained_model.load_state_dict(torch.load(\"../Model/fine_tuned_bert.pth\", map_location=device))\n",
    "\n",
    "pretrained_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tuned_embeddings(data_loader):\n",
    "    pretrained_model.eval()  # Set to evaluation mode\n",
    "    all_labels = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            labels = data['targets'].cpu().numpy()\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            # Get embeddings from the fine-tuned model\n",
    "            outputs = pretrained_model.pretrainedLayer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "            embeddings = outputs['pooler_output']  # Shape: [batch_size, 768]\n",
    "            embeddings_list.append(embeddings.cpu().numpy())\n",
    "            \n",
    "    # Concatenate all embeddings into a single numpy array\n",
    "    all_embeddings = np.vstack(embeddings_list)\n",
    "    return np.array(all_embeddings), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = CustomDataset(dataframe=df, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "inference_params = {\n",
    "    'batch_size': INFERENCE_BATCH_SIZE, \n",
    "    'shuffle': False,\n",
    "    'num_workers': 0 \n",
    "}\n",
    "inference_loader=DataLoader(inference_dataset, **inference_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (15517, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_fine_tuned_embeddings(inference_loader)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "cosine_sim_matrix = cosine_similarity(embeddings)\n",
    "cosine_dist_matrix = 1 - cosine_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements assigned to each cluster: [3699  964 4343 2460 1949 2102]\n",
      "[2 0 5 ... 1 3 1]\n",
      "Number of elements assigned to each cluster: [1166 3446 2406 3738 2152 2609]\n",
      "[1 5 4 ... 0 1 0]\n",
      "Number of elements assigned to each cluster: [3732 2612 1166 3436 2404 2167]\n",
      "[3 1 5 ... 2 3 2]\n",
      "Number of elements assigned to each cluster: [2617 3460 3743 2148 2388 1161]\n",
      "[1 0 3 ... 5 1 5]\n",
      "Number of elements assigned to each cluster: [ 953 4343 1977 3700 2094 2450]\n",
      "[1 3 4 ... 0 5 0]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=NUM_CLUSTERS,\n",
    "        max_iter=100,\n",
    "        init='k-means++',\n",
    "        n_init=\"auto\",\n",
    "        random_state=seed,\n",
    "    ).fit(embeddings)\n",
    "\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    accuracy = metrics.accuracy_score(y_true, cluster_labels, normalize=True, sample_weight=None)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    nmi = metrics.normalized_mutual_info_score(labels_true, cluster_labels, average_method='arithmetic')\n",
    "    print(\"NMI: \", nmi)\n",
    "    ari = metrics.adjusted_rand_score(labels_true, cluster_labels)\n",
    "    print(\"ARI: \", ari)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
