{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from torch import cuda\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "INFERENCE_BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 3e-5\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_CLUSTERS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing and returning data in a format that can be directly used for training the model\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text_description = dataframe.TextDescription\n",
    "        self.targets = dataframe.EncodedSeverity\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_description)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_description = str(self.text_description[index])\n",
    "        text_description = \" \".join(text_description.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text_description,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a linear layer on top of BERT to get the final output for the model. \n",
    "class BERTClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.pretrainedLayer = transformers.BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropOutLayer = torch.nn.Dropout(0.3)\n",
    "        self.linearLayer = torch.nn.Linear(768, 6)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output_1= self.pretrainedLayer(input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.dropOutLayer(output_1)\n",
    "        output = self.linearLayer(output_2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (pretrainedLayer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropOutLayer): Dropout(p=0.3, inplace=False)\n",
       "  (linearLayer): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializes the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading, preparing, and splitting the dataset into a testing and training set\n",
    "df = pd.read_csv('../DataSet/CleanDataSet.csv')\n",
    "\n",
    "train_size = 0.7\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the DataLoader objects for training and testing datasets, specifying how the data should be loaded during training\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapting weights to balance an unbalanced dataset \n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=\n",
    "                                                 np.unique(df.EncodedSeverity),\n",
    "                                                 y=df.EncodedSeverity)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(class_weights)(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Adam Optimizer to optimize the loss\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for training the model\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    fin_outputs = []\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "    return fin_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  1.8011667728424072\n",
      "Epoch: 1, Loss:  1.9206212759017944\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for validating the training\n",
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.argmax(outputs, dim=1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    print(\"Sample Predictions:\", fin_outputs[:20])  # Print first 20 predictions\n",
    "    print(\"Actual Labels:\", fin_targets[:20]) \n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Actual Labels: [0, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 1, 4, 2, 4, 4, 4, 4]\n",
      "Accuracy Score = 0.827922077922078\n",
      "Balanced Accuracy Score = 0.16666666666666666\n",
      "F1 Score (Micro) = 0.827922077922078\n",
      "F1 Score (Macro) = 0.15097690941385436\n",
      "Sample Predictions: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Actual Labels: [0, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Accuracy Score = 0.827922077922078\n",
      "Balanced Accuracy Score = 0.16666666666666666\n",
      "F1 Score (Micro) = 0.827922077922078\n",
      "F1 Score (Macro) = 0.15097690941385436\n"
     ]
    }
   ],
   "source": [
    "# Validate the training\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs)\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    b_accuracy = metrics.balanced_accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"Balanced Accuracy Score = {b_accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the finetuned model\n",
    "torch.save(model.state_dict(), \"../Model/fine_tuned_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the pretrained model\n",
    "inference_model = BERTClass()\n",
    "inference_model.to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "inference_model.load_state_dict(torch.load(\"../Model/best_fine_tuned_bert.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for getting embeddings and their respective truth label\n",
    "def get_fine_tuned_embeddings(data_loader):\n",
    "    inference_model.eval()\n",
    "    all_truth_labels = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)\n",
    "            \n",
    "            # Extracting truth labels and converting to a 1d list\n",
    "            labels = data['targets'].cpu().numpy()\n",
    "            for label in labels:\n",
    "                all_truth_labels.append(int(label))\n",
    "\n",
    "            # Get embeddings from the fine-tuned model\n",
    "            outputs = inference_model.pretrainedLayer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "            embeddings = outputs['pooler_output'] \n",
    "            embeddings_list.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all embeddings into a single numpy array\n",
    "    all_embeddings = np.vstack(embeddings_list)\n",
    "    return all_embeddings, all_truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inference dataloader\n",
    "inference_dataset = CustomDataset(dataframe=df, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "inference_params = {\n",
    "    'batch_size': INFERENCE_BATCH_SIZE, \n",
    "    'shuffle': False,\n",
    "    'num_workers': 0 \n",
    "}\n",
    "inference_loader=DataLoader(inference_dataset, **inference_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting embeddings and their truth labels\n",
    "embeddings, all_truth_labels = get_fine_tuned_embeddings(inference_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the embeddings\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "from scipy.stats import zscore\n",
    "# Calculate Z-scores for each embedding\n",
    "z_scores = np.abs(zscore(scaled_embeddings, axis=0))\n",
    "\n",
    "# Set a threshold (commonly 3 or higher for high-dimensional data)\n",
    "threshold = 3\n",
    "\n",
    "# Identify outliers (if any z-score across dimensions exceeds the threshold)\n",
    "outliers = np.where(np.any(z_scores > threshold, axis=1))\n",
    "\n",
    "# Remove outliers from embeddings\n",
    "cleaned_embeddings = np.delete(scaled_embeddings, outliers[0], axis=0)\n",
    "print(f\"Removed {len(outliers[0])} outliers using Z-score method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements assigned to each cluster: [5065    1  381 1621 4377 3956]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of elements assigned to each cluster: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m cluster_labels = kmeans.fit_predict(embeddings)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m single_point_clusters = [cluster \u001b[38;5;28;01mfor\u001b[39;00m cluster, size \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcluster_sizes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m size == \u001b[32m1\u001b[39m]\n\u001b[32m     29\u001b[39m df = df[np.isin(cluster_labels, single_point_clusters, invert=\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[32m     33\u001b[39m nmi = metrics.normalized_mutual_info_score(all_truth_labels, cluster_labels, average_method=\u001b[33m'\u001b[39m\u001b[33marithmetic\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# Executes Kmeans and evaluates the clustering results\n",
    "kmeans=[]\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "cluster_labels = []\n",
    "\n",
    "for seed in range(5):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=NUM_CLUSTERS,\n",
    "        max_iter=100,\n",
    "        init='k-means++',\n",
    "        n_init=\"auto\",\n",
    "        random_state=seed,\n",
    "    ).fit(embeddings)\n",
    "\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "    # single_point_clusters = [cluster for cluster, size in cluster_sizes.items() if size == 1]\n",
    "    # df = df[np.isin(cluster_labels, single_point_clusters, invert=True)]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    nmi = metrics.normalized_mutual_info_score(all_truth_labels, cluster_labels, average_method='arithmetic')\n",
    "    print(\"NMI: \", nmi)\n",
    "    ari = metrics.adjusted_rand_score(all_truth_labels, cluster_labels)\n",
    "    print(\"ARI: \", ari)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Standardize the embeddings\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    " \n",
    "\n",
    "# Now apply PCA to the standardized embeddings\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_result = pca.fit_transform(scaled_embeddings)\n",
    "\n",
    "df_visual = pd.DataFrame(pca_result, columns=['PCA1', 'PCA2'])\n",
    "df_visual['Cluster'] = cluster_labels\n",
    "\n",
    "# sns.scatterplot(x=\"PCA1\", y=\"PCA2\", data=df_visual, hue=\"Cluster\")\n",
    "sns.boxplot(data=df_visual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
